import os
import uuid
import torch
from PIL import Image
from typing import List, Tuple
from app.config import logger, envs
from app.config.model_config import (
    QWEN_MODEL_NAME,
    DEVICE,
    TORCH_DTYPE,
    USE_LIGHTNING_LORA,
    LIGHTNING_LORA_PATH
)


class ImageLayeredService:
    """ì´ë¯¸ì§€ ë ˆì´ì–´ ë¶„í•´ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.pipeline = None
        self.device = DEVICE
        self.output_dir = envs.OUTPUT_DIR or "outputs"
        os.makedirs(self.output_dir, exist_ok=True)

    async def load_model(self):
        """ML ëª¨ë¸ ë¡œë”©"""
        try:
            from diffusers import QwenImageLayeredPipeline

            logger.info(f"Loading Qwen Image Layered model on device: {self.device}")
            logger.info(f"Using torch dtype: {TORCH_DTYPE}")

            # CUDA í™˜ê²½: 4-bit ì–‘ìí™” ì‚¬ìš©
            if self.device == "cuda":
                from diffusers import PipelineQuantizationConfig

                logger.info("âœ… CUDA GPU detected - using 4-bit quantization")
                logger.info("ğŸ”§ Memory usage: 57.7GB â†’ ~15GB with quantization")

                quantization_config = PipelineQuantizationConfig(
                    quant_backend="bitsandbytes_4bit",
                    components_to_quantize=["transformer", "text_encoder"],
                    quant_kwargs={
                        "load_in_4bit": True,
                        "bnb_4bit_compute_dtype": TORCH_DTYPE,
                        "bnb_4bit_quant_type": "nf4",
                        "bnb_4bit_use_double_quant": True,
                    }
                )

                self.pipeline = QwenImageLayeredPipeline.from_pretrained(
                    QWEN_MODEL_NAME,
                    torch_dtype=TORCH_DTYPE,
                    quantization_config=quantization_config,
                )

            # MPS/CPU: ì–‘ìí™” ì—†ì´ ë¡œë“œ (bitsandbytes ë¯¸ì§€ì›)
            else:
                if self.device == "mps":
                    logger.warning("âš ï¸  Apple Silicon (MPS) - quantization not supported")
                    logger.warning("âš ï¸  Loading full model (~60GB memory, may use swap)")
                else:  # CPU
                    logger.warning("âš ï¸  Running on CPU - will be very slow")
                    logger.warning("âš ï¸  Loading full model (~60GB memory, may use swap)")

                self.pipeline = QwenImageLayeredPipeline.from_pretrained(
                    QWEN_MODEL_NAME,
                    torch_dtype=TORCH_DTYPE,
                    low_cpu_mem_usage=True,
                )

            # MPS: ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ì‹œë„ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‹¤íŒ¨ ê°€ëŠ¥)
            if self.device == "mps":
                logger.info(f"Attempting to move to {self.device}...")
                try:
                    self.pipeline = self.pipeline.to(self.device)
                    logger.info(f"âœ… Successfully moved to {self.device}")
                except Exception as e:
                    logger.error(f"Failed to move to {self.device}: {e}")
                    logger.warning("Falling back to CPU")
                    self.device = "cpu"

            # ì„ íƒ: Lightning LoRA ë¡œë“œ
            if USE_LIGHTNING_LORA:
                self.pipeline.load_lora_weights(LIGHTNING_LORA_PATH)
                logger.info(f"Lightning LoRA loaded: {LIGHTNING_LORA_PATH}")

            logger.info("Model loaded successfully!")
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise

    async def decompose_image(
        self,
        image: Image.Image,
        layers: int = 4,
        resolution: int = 640,
        num_inference_steps: int = 50,
        true_cfg_scale: float = 4.0,
        seed: int = 42
    ) -> Tuple[str, List[str], int]:
        """
        ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ë ˆì´ì–´ë¡œ ë¶„í•´

        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            layers: ìƒì„±í•  ë ˆì´ì–´ ìˆ˜
            resolution: ì¶œë ¥ í•´ìƒë„
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜ (LoRA ì‚¬ìš© ì‹œ 8)
            true_cfg_scale: CFG ìŠ¤ì¼€ì¼
            seed: ëœë¤ ì‹œë“œ

        Returns:
            (result_id, layer_paths, count)
        """
        if self.pipeline is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")

        try:
            # RGBAë¡œ ë³€í™˜
            image = image.convert("RGBA")

            # ì¶”ë¡ 
            with torch.inference_mode():
                output = self.pipeline(
                    image=image,
                    layers=layers,
                    resolution=resolution,
                    num_inference_steps=num_inference_steps,
                    true_cfg_scale=true_cfg_scale,
                    generator=torch.Generator(device=self.device).manual_seed(seed)
                )

            # ê²°ê³¼ ì €ì¥
            result_id = str(uuid.uuid4())[:8]
            paths = []

            for i, layer in enumerate(output.images[0]):
                filename = f"{result_id}_layer{i}.png"
                file_path = os.path.join(self.output_dir, filename)
                layer.save(file_path)
                paths.append(filename)
                logger.info(f"Saved layer {i} to {file_path}")

            return result_id, paths, len(paths)

        except Exception as e:
            logger.error(f"Image decomposition failed: {e}")
            raise

    def get_file_path(self, filename: str) -> str:
        """íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        file_path = os.path.join(self.output_dir, filename)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {filename}")
        return file_path


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
image_layered_service = ImageLayeredService()
